{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761f972c",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing and feature engineering are critical steps in the machine learning pipeline. They involve preparing raw data for analysis and transforming it into a format that can be effectively used by machine learning algorithms. How is this diiferent from EDA? Well, EDA (Exploratory Data Analysis) focuses on understanding the data, identifying patterns, and uncovering insights through visualization and statistical analysis. In contrast, data preprocessing and feature engineering are more about cleaning the data, standardizing or normalizing it and creating new features to improve model performance.\n",
    "\n",
    "EDA -> Understanding the data, identifying patterns, uncovering insights.\n",
    "\n",
    "Data Preprocessing and Feature Engineering -> Cleaning the data, standardizing/normalizing, creating new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b14c5d",
   "metadata": {},
   "source": [
    "## Data Preprocessing Steps\n",
    "\n",
    "##### Step 1: Handling Missing Values\n",
    "\n",
    "- Identify missing values in the dataset.\n",
    "- Decide on a strategy to handle them (e.g., removal, imputation).\n",
    "- If imputing, choose an appropriate method (mean, median, mode, or predictive modeling).[Use median for numerical data to avoid outlier impact, mean for normally distributed data, and mode for categorical data.]\n",
    "\n",
    "```python\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df['column_having_missing_values'].fillna(df['column_having_missing_values'].median())\n",
    "\n",
    "# or for categorical data\n",
    "df['categorical_column'].fillna(df['categorical_column'].mode()[0])\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "```\n",
    "\n",
    "##### Step 2: Encoding Categorical Variables\n",
    "\n",
    "- Convert categorical variables into numerical format.\n",
    "- Use techniques like one-hot encoding(for non-hierarchical categories), label encoding(for yes/no categories), or ordinal encoding(for hierarchical categories) based on the nature of the categorical data.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# One-Hot Encoding\n",
    "pd.get_dummies(df, columns=categorical_columns, dtype=int)      #It generates true/false columns for each category, so specify dtype=int to convert them to 0/1\n",
    "```\n",
    "\n",
    "##### Step 3: Scaling and Normalization\n",
    "\n",
    "- Check if your dataset have features with different range of values or not. Then take decision on whether scaling is required or not\n",
    "- Apply scaling techniques like Min-Max Scaling or Standardization (Z-score normalization) to ensure all features contribute equally to the model.  \n",
    "- If required apply one of the scaling methods.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "target = 'target_column'\n",
    "X = df.drop(columns=[target])           # Select all columns except target column\n",
    "y = df[target]                  # Select only target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)           # Split the data into training and testing sets. How the whole process is working here? We are splitting the data into training and testing sets before scaling to avoid data leakage. Scaling should be done after the split to ensure that the model is trained on data that is representative of real-world scenarios. Here test_size=0.25 means 25% data will be used for testing and 75% data will be used for training, random_state=42 is used to ensure reproducibility of the results. Reproducibility means that every time the code runs with the same random_state, it will produce the same split of data. Why 42? Because it's a commonly used arbitrary number in programming and data science.\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit to training data and transform it\n",
    "X_test_scaled = scaler.transform(X_test)        # Transform test data\n",
    "\n",
    "# fit_transform() is used on training data to compute the mean and standard deviation, and then transform the data. transform() is used on test data to apply the same transformation using the parameters learned from the training data.\n",
    "\n",
    "# Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)  # Fit to training data and transform it\n",
    "X_test_minmax = min_max_scaler.transform(X_test)        # Transform test data\n",
    "\n",
    "# Convert scaled arrays back to DataFrames for easier handling\n",
    "# Standardization \n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Min-Max Scaling\n",
    "X_train_minmax = pd.DataFrame(X_train_minmax, columns=X_train.columns, index=X_train.index)\n",
    "X_test_minmax = pd.DataFrame(X_test_minmax, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "display(X_train_scaled.head())\n",
    "display(X_test_scaled.head())\n",
    "display(X_train_minmax.head())\n",
    "display(X_test_minmax.head())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
