{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba86ab90",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis(EDA) and Preprocessing\n",
    "\n",
    "EDA deals with understanding the data, identifying patterns, and uncovering insights by visualizing and summarizing the data. Preprocessing involves cleaning and transforming the data to make it suitable for analysis or modeling. Both EDA and preprocessing are crucial steps in any ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128c9f9",
   "metadata": {},
   "source": [
    "### Key things to keep in mind when selecting dataset from Kaggle/UCI/Google Dataset or any other source:\n",
    "\n",
    "1. Usability: Must be above 7/7.5 out of 10.\n",
    "2. Labels: Must have clear labels.\n",
    "3. Source & Acknowledgment: Must have clear source and acknowledgment of creator.\n",
    "4. Citation: If the dataset is used in any publication, it's a reliable source.\n",
    "5. Synthetic vs Real: Synthetic datasets are generated artificially or copied from other sources, while real datasets are collected from real-world observations. Real datasets are generally preferred for training ML models as they better represent real-world scenarios. It is found in the context of that particular dataset.\n",
    "6. Overall Summary: Read the overall summary of the dataset to understand its context and purpose.\n",
    "7. Attributes: Check the attributes/features of the dataset to see if they are relevant to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5006fb",
   "metadata": {},
   "source": [
    "## EDA Steps:\n",
    "\n",
    "#### Step 1: Import Libraries and Load Dataset\n",
    "---\n",
    "\n",
    "At this point basic things like how many rows and columns, what are the column names, data types, missing values, and basic statistics should be checked.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "sns.set(font_scale=1.1)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"path_to_your_dataset.csv\")\n",
    "\n",
    "# Get basic info\n",
    "print(\"Shape: \", df.shape)\n",
    "print(\"Columns: \", df.columns)\n",
    "print(\"Info: \")\n",
    "print(df.info())\n",
    "print(\"Description: \")\n",
    "print(df.describe().T)\n",
    "```\n",
    "\n",
    "#### Step 2: Define Target & Features Types\n",
    "---\n",
    "\n",
    "Explicitly listing numeric and categorical features makes the work reproducible. It also forces one to think about each column and its meaning, which is a key habit in serious ML projects.\n",
    "\n",
    "```python\n",
    "\n",
    "target_col = \"target_column_name\"  # Replace with your target column name\n",
    "feature_cols = [col for col in df.columns if col != target_col]\n",
    "numerical_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Target Column: \", target_col)\n",
    "print(\"Numerical Columns: \", numerical_cols)\n",
    "print(\"Categorical Columns: \", categorical_cols)\n",
    "```\n",
    "\n",
    "#### Step 3: Missing Values & Basic Quality Checks\n",
    "---\n",
    "\n",
    "Look for impossible or suspicious ranges such as zero or extremely low or high & missing values. These may be data entry issues or special codes that need to be handled separately. Even if the dataset description claims there are no missing values, always verify. Real world data often breaks promises. Also, check for unique values in categorical columns to identify inconsistencies.\n",
    "\n",
    "```python\n",
    "df.isna().sum()         # Check for missing values per column\n",
    "df[numerical_cols].agg(['min', 'max', 'mean', 'median', 'std'])  # Basic stats for numerical columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(col, df[col].unique())  # Unique values in categorical columns. Like in Sex column we can get M, F, Female, Male, 'F' etc., which we can later preprocess by combining them.\n",
    "```\n",
    "\n",
    "#### Step 4: Understanding Distributions with Histograms and Boxplots\n",
    "---\n",
    "\n",
    "Histograms tell the shape of distributions. Boxplots give a quick view of spread and potential outliers. Before any scaling or transformation, a mental picture of how these variables behave is important.\n",
    "\n",
    "```python\n",
    "# Histograms for numerical features\n",
    "df[numerical_cols].hist(bins=15, figsize=(12,8))\n",
    "plt.suptitle(\"Histograms of Numerical Features\", fontsize=14)\n",
    "plt.tight_layout()          # Adjust layout to prevent different graphs overlap\n",
    "plt.show()\n",
    "\n",
    "# Single column histogram \n",
    "df['numerical_column_name'].hist(bins=15, figsize=(6,4))\n",
    "plt.title(\"Histogram of Numerical Column\", fontsize=14)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for numerical features\n",
    "plt.figure(figsize=(12,8))\n",
    "df[numerical_cols].boxplot()\n",
    "plt.title(\"Boxplots of Numerical Features\", fontsize=14)\n",
    "plt.xticks(rotation=45)         # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "# Single column boxplot\n",
    "plt.figure(figsize=(6,4))\n",
    "df.boxplot(column='numerical_column_name')\n",
    "plt.title(\"Boxplot of Numerical Column\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 5: Target Distribution and Class Imbalance\n",
    "---\n",
    "\n",
    "See how balanced the target variable is. If one class dominates heavily, it needs to resampling strategies or class weighted models later. Even if the imbalance is moderate, it is critical to be aware of it before modeling.\n",
    "\n",
    "```python\n",
    "# Target variable distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=target_col, data=df)\n",
    "plt.title(\"Target Variable Distribution\", fontsize=14)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Normalized distribution\n",
    "df[target_col].value_counts(normalize=True)         # Proportion of each class in target variable in scale of 100% \n",
    "```\n",
    "\n",
    "#### Step 6: Categorical Feature Exploration\n",
    "---\n",
    "\n",
    "For categorical variables, check if some categories have very few samples. Rare categories can be merged, encoded carefully, or sometimes dropped if they add noise instead of signal.\n",
    "\n",
    "```python\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f\"Population of {col}\", fontsize=14)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)         # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "#### Step 7:  Relationships Between Features and Target\n",
    "---\n",
    "\n",
    "The relationship between numerical features and the target variable can be visualized using boxplots. This helps in understanding how different values of the target variable affect the distribution of numerical features. Large differences in distributions between target classes often signal strong predictive potential. If the distributions are almost identical, that feature may be less useful on its own.\n",
    "\n",
    "```python\n",
    "# Boxplots of numerical features vs target\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, col in enumerate(numerical_cols,1):          # 1-based index for subplot\n",
    "    plt.subplot(2, 3, i)            # 2 rows, 3 columns of subplots\n",
    "    sns.boxplot(x=target_col, y=col, data=df)\n",
    "    plt.title(f\"{col} vs {target_col}\", fontsize=12)\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 8: Pairplot for a Subset of Features\n",
    "---\n",
    "\n",
    "Pairplots are expensive but powerful for small to medium sized datasets. They show pairwise relationships and distributions in one go.\n",
    "\n",
    "```python\n",
    "sns.pairplot(df[numerical_cols], hue=target_col, diag_kind='hist')\n",
    "plt.suptitle(\"Pairplot of Numerical Features\", fontsize=14)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Step 9: Correlation Matrix and Heatmap\n",
    "---\n",
    "\n",
    "High absolute correlation with the target is interesting, but low correlation features cann't be ignored. Some of them can become powerful in combination with others. Also watch for high correlation between predictors, which may indicate redundancy.\n",
    "\n",
    "```python\n",
    "corr_matrix = df[numerical_cols + [target_col]].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"Correlation Matrix\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr_matrix[target_col].sort_values(ascending=False)  # Sorting correlation of features with target variable\n",
    "```\n",
    "\n",
    "#### Step 10: Categorical Feature vs Target\n",
    "---\n",
    "\n",
    "Identify categorical features that have strong relationships with the target variable using stacked bar plots & crosstabs. Crosstabs normalized by row are very helpful. They show how the proportions change across categories of the categorical feature. This is often more informative than raw counts.\n",
    "\n",
    "```python\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    \n",
    "    ct= pd.crosstab(df[col], df[target_col], normalize='index')   # Normalized crosstab to see proportions instead of counts\n",
    "    ct.plot(kind='bar', stacked=True)\n",
    "    plt.title(f\"{col} vs {target_col}\", fontsize=14)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=0)         # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a8916",
   "metadata": {},
   "source": [
    "### 8. EDA Best Practices Before Model Building\n",
    "\n",
    "Some practical guidelines:\n",
    "\n",
    "- Read the dataset description and understand dataset context where possible  \n",
    "- Verify data types and ranges, do not trust them blindly  \n",
    "- Check missing values and consider if they are random or systematic  \n",
    "- Study distributions, not just summary statistics  \n",
    "- Look at target distribution and potential class imbalance  \n",
    "- Explore feature target relationships through plots and simple statistics  \n",
    "- Take notes about:\n",
    "  - Features that look noisy or suspicious  \n",
    "  - Features that seem strongly related to the target  \n",
    "  - Possible transformations such as log, binning, or scaling  \n",
    "  - Any domain inspired features you might create later  \n",
    "\n",
    "> Insight: Good EDA is like building a mental simulation of how the data behaves. Once that simulation is clear, the choices in preprocessing and modeling feel much less random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
