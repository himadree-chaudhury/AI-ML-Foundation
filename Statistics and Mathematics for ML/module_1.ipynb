{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6fb6ae",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "### What is Statistics?\n",
    "Statistics is the science of working with data - **collecting**, **summarizing**, analyzing, and interpreting it to understand the real world through numbers.\n",
    "Example:\n",
    "- Suppose we collect the marks of all students in a class.\n",
    "- The **mean** tells us the class’s average performance, the **median** shows the middle score, and the **standard deviation** tells us how spread out the marks are.\n",
    "That’s statistics - finding meaning in data.\n",
    "\n",
    "### What is Statistics for Machine Learning?\n",
    "Statistics for ML helps us **understand the data before we train models.**\n",
    "A model learns patterns from data, so if we don’t understand our data’s behavior, we risk building biased or misleading models.\n",
    "We use statistics in ML to study **the center**, **spread**, **shape**, and **relationships** within data.\n",
    "It forms the foundation for preprocessing, feature scaling, and model evaluation.\n",
    "\n",
    "### Types of Statistics\n",
    "There are two main types of statistics:\n",
    "1. **Descriptive Statistics** - describe or summarize data.\n",
    "   Example: the average mark of a class is 78.\n",
    "2. **Inferential Statistics** - draw conclusions about a population from a sample.\n",
    "   Example: using one class’s marks to estimate the average for the whole university.\n",
    "\n",
    "In machine learning, we mostly rely on descriptive statistics because we focus on understanding and preparing our dataset, not on generalizing to an unseen population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423a7eb",
   "metadata": {},
   "source": [
    "## Descriptive Statistics and Distributions\n",
    "\n",
    "Descriptive statistics helps us summarize and visualize data using numbers and graphs.\n",
    "We’ll learn how to describe data’s center, spread, and shape.\n",
    "\n",
    "### Measures of Central Tendency\n",
    "1. **Mean** - The average value. Good for normal, balanced data.\n",
    "    Formula: 1/N Σ(xi)\n",
    "2. **Median** - The middle value when data is sorted. Good for skewed or outlier-rich data.\n",
    "    Formula: (N is odd) middle value; (N is even) average of two middle values.\n",
    "3. **Mode** - The most frequent value. Mainly used for categorical data.\n",
    "\n",
    "| Situation               | Best Measure       | Reasoning                          |\n",
    "|-------------------------|--------------------|------------------------------------|\n",
    "| Symmetric Distribution (e.g. height, temperature)   | Mean               | All values contribute equally.     |\n",
    "| Skewed Distribution (e.g. income, house prices)     | Median             | Less affected by outliers.         |\n",
    "| Categorical Data (e.g. color, brand)                 | Mode               | Shows most common category & non-numeric.        |\n",
    "\n",
    "**Common Mistakes**\n",
    "1. Using mean with skewed data → misleading\n",
    "2. Using median for categorical → meaningless\n",
    "3. Using mode for continuous numeric → rarely helpful\n",
    "4. Forgetting to check distribution before imputing missing values.\n",
    "\n",
    "### Measures of Spread\n",
    "1. **Range** - The difference between the maximum and minimum values.\n",
    "2. **Variance** - The average of the squared differences from the mean.\n",
    "    i. **Population Variance(σ²)** - Used when you have data for the entire population.\n",
    "    Formulas: 1/N Σ (xi - x̄)²\n",
    "   ii. **Sample Variance(s²)** - Used when you have a sample from a larger population.\n",
    "    Formula: 1/(N-1) Σ (xi - x̄)², here N-1 is Bessel's correction.\n",
    "3. **Standard Deviation** - The square root of the variance.\n",
    "    i. **Population Standard Deviation(σ)** - Square root of population variance.\n",
    "    ii. **Sample Standard Deviation(s)** - Square root of sample variance.\n",
    "\n",
    "**Variance & SD in ML Pipeline**\n",
    "1. Feature Scaling:Standardization uses to make features comparable by ensuring that each feature has mean = 0 and standard deviation = 1.\n",
    "    Formula: z = (x - μ) / σ\n",
    "2. Regularization:Ridge and Lasso reduce coefficient variance to prevent overfitting.\n",
    "3. Model Diagnostics:High variance in model performance across folds suggests instability.\n",
    "\n",
    "\n",
    "**Bias-Variance Trade-off**\n",
    "\n",
    "In model evaluation, variance also refers to how much a model’s predictions change with different training data.\n",
    "A high-variance model memorizes the training set (overfitting), while low variance but high bias underfits. So, variance of data and variance of model parameters are different concepts but share the same intuition —instability due to spread.\n",
    "\n",
    "| Concept                | Variance (σ²)                       | Standard Deviation (σ)         |\n",
    "|------------------------|-------------------------------------|---------------------------------|\n",
    "| Definition              | Average of squared deviations       | Square root of variance                         |\n",
    "| Formula                 | σ² = 1/N Σ (xi - μ)²               | σ = √σ²                         |\n",
    "| Units                   | Squared units of data               | Same units as data              |\n",
    "| Interpretation          | Mathematical measure                | Intuitive measure(means decision reasoning impact)               |\n",
    "| Use Case                | Theoretical/ statistical analysis    | Communication & scaling        |\n",
    "\n",
    "\n",
    "### Data Distributions\n",
    "Understanding how data is distributed is crucial for analysis.\n",
    "1. **Normal Distribution** - Bell-shaped curve, symmetric about the mean.\n",
    "2. **Skewed Distribution** - Data is not symmetric; can be left or right-skewed.\n",
    "3. **Bimodal Distribution** - Two peaks in the data.\n",
    "\n",
    "Visualizations like histograms and box plots help us see these distributions clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06745867",
   "metadata": {},
   "source": [
    "## Understanding Percentiles, Quartiles, IQR, and Z-Score in Machine Learning\n",
    "\n",
    "### Percentiles\n",
    "Percentile tells us the value below which a given percentage of observations fall.\n",
    "For example, the 70th percentile is the value below which 70% of the observations may be found.\n",
    "    Formula: Percentile rank = (number of values below x / total number of values) × 100\n",
    "\n",
    "### Quartiles\n",
    "Quartiles divide data into four equal parts:\n",
    "1. Q1 (25th percentile) - 25% of data below this value.\n",
    "2. Q2 (50th percentile/median) - 50% of data below this value.\n",
    "3. Q3 (75th percentile) - 75% of data below this value.\n",
    "\n",
    "We often use percentiles to handle outliers in data.\n",
    "For example, when preprocessing numerical features, we can remove values below the 1st percentile or above the 99th percentile to reduce the effect of extreme data points.\n",
    "\n",
    "### Interquartile Range (IQR)\n",
    "IQR is the range between Q1 and Q3 and represents the middle 50% of the data.\n",
    "    Formula: IQR = Q3 - Q1\n",
    "\n",
    "- A smaller IQR means data points are tightly packed.\n",
    "- A larger IQR means data has high variability.\n",
    "\n",
    "**Outlier Detection Rule**:\n",
    "Any point below Q1 − 1.5 × IQR or above Q3 + 1.5 × IQR is an outlier.\n",
    "Here 1.5 is a commonly used multiplier, but it can be adjusted based on the specific dataset and context.\n",
    "\n",
    "IQR-based filtering is common before training models to prevent extreme values from skewing results, for example, in housing price prediction, or salary datasets, where a few very large values can distort the model’s understanding of normal behavior.\n",
    "\n",
    "### Z-Score\n",
    "Z-score measures how many standard deviations a data point is from the mean.\n",
    "    Formula: Z = (X - μ) / σ\n",
    "Where:\n",
    "- X is the value\n",
    "- μ is the mean\n",
    "- σ is the standard deviation\n",
    "- A Z-score of 0 means the value is exactly at the mean.\n",
    "- A positive Z-score means the value is above the mean.\n",
    "- A negative Z-score means the value is below the mean.\n",
    "\n",
    "Z-score normalization is part of standard scaling, which transforms all features to have mean 0 and standard deviation 1.\n",
    "This is critical for models like KNN, SVM, or Gradient Descent-based models, which are sensitive to feature scale.\n",
    "\n",
    "\n",
    "|Concept          | Definition                                      | Formula                             | Use Case in ML Pipeline                      |\n",
    "|-----------------|-------------------------------------------------|-------------------------------------|----------------------------------------------|\n",
    "| Percentiles     | Value below which a certain percentage of data falls | Percentile rank = (number of values below x / total number of values) × 100 | Handling outliers by trimming extreme values |\n",
    "| Quartiles       | Values that divide data into four equal parts   | Q1 (25th percentile), Q2 (50th percentile/median), Q3 (75th percentile) | Summarizing data distribution and spread     |\n",
    "| IQR             | Range between Q1 and Q3 representing middle 50% of data | IQR = Q3 - Q1                        | Outlier detection and removal               |\n",
    "| Z-Score         | Number of standard deviations a data point is from the mean | Z = (X - μ) / σ                  | Feature scaling for distance-based models    | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d9632",
   "metadata": {},
   "source": [
    "### If standard deviation already tells how far something sits from the mean, why drag in another number like z-score?\n",
    "\n",
    "Standard deviation gives the distance. If standard deviation says the value is, say, 12 units above the mean.\n",
    "\n",
    "But that “12” is stuck to the scale of the data.\n",
    "- If the data is in kilograms, it’s 12 kg.\n",
    "- If the data is in exam marks, it’s 12 marks.\n",
    "\n",
    "If the data has a different standard deviation, that “12” might be huge or tiny. So SD is in the same unit as the dataset. As a result data can’t reliably compare across datasets. \n",
    "\n",
    "Z-score gives the meaning. Z-score asks: How many standard deviations away from the mean are you?\n",
    "\n",
    "- Z = +2 means “two standard deviations above average,” whether it is measuring height, income, marks, rainfall, or CPU temperature.\n",
    "- Z = –1 means “one standard deviation below average,” same interpretation everywhere.\n",
    "\n",
    "That makes it unitless and comparable anywhere. It puts everything on a universal scale. \n",
    "\n",
    "* SD → describes the spread of the entire dataset in original units.\n",
    "* Z → describes the value’s position relative to the mean, in units of SD.\n",
    "\n",
    "#### Example\n",
    "If SD = 12 and Z = 12, then the actual deviation from the mean is:\n",
    "12 (SD) × 12 (Z) = 144 units above the mean.\n",
    "So the SD tells the “size” of one step and z-score tells how many steps is moved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae5381",
   "metadata": {},
   "source": [
    "### The problems of IQR and Z-Score\n",
    "\n",
    "Both IQR and Z-Score are popular methods for detecting outliers, but they have limitations:\n",
    "1. **IQR Limitations**:\n",
    "   - Not effective for small datasets: In small datasets, the quartiles may not be well-defined, leading to unreliable IQR calculations.\n",
    "   - Assumes a symmetric distribution: IQR works best with symmetric distributions; in highly skewed data, it may misclassify normal points as outliers.\n",
    "   - Depends on Q3 and Q1: If these quartiles are affected by outliers, the IQR method may fail to identify true outliers.\n",
    "2. **Z-Score Limitations**:\n",
    "   - Assumes normal distribution: Z-score is most effective when the data follows a normal distribution. In skewed distributions, it may misidentify outliers.\n",
    "   - Sensitive to mean and standard deviation: Extreme values can distort the mean and standard deviation, leading to misleading z-scores.\n",
    "   - Not robust for small samples: In small datasets, z-scores can be unreliable due to high variability in mean and standard deviation estimates.\n",
    "\n",
    "To solve these problems, we use robust methods like the Modified Z-Score or the Median Absolute Deviation (MAD) for outlier detection in skewed or small datasets.\n",
    "\n",
    "### Modified Z-Score\n",
    "The Modified Z-Score is a robust alternative to the traditional z-score, especially useful for small or skewed datasets. It uses the median and the Median Absolute Deviation (MAD) instead of the mean and standard deviation.\n",
    "Formula:\n",
    "Modified Z = 0.6745 * (X - Median) / MAD\n",
    "Where:\n",
    "- X is the value\n",
    "- Median is the median of the dataset\n",
    "- MAD is the Median Absolute Deviation\n",
    "\n",
    "### Median Absolute Deviation (MAD)\n",
    "MAD is a robust measure of variability that is less affected by outliers than standard deviation.\n",
    "Formula:\n",
    "MAD = median(|Xi - Median|)\n",
    "Where:\n",
    "- Xi is each value in the dataset\n",
    "- Median is the median of the dataset\n",
    "\n",
    "By using the Modified Z-Score and MAD, we can more effectively identify outliers in datasets that are not normally distributed or contain extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cb4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 27.5 Q3: 85.5 IQR: 58.0\n",
      "Lower Bound: -30.5 Upper Bound: 143.5\n",
      "Outliers using IQR:\n",
      " Empty DataFrame\n",
      "Columns: [Income]\n",
      "Index: []\n",
      "Outliers using Z-score:\n",
      " Empty DataFrame\n",
      "Columns: [Income, z_score]\n",
      "Index: []\n",
      "Outliers using Modified Z-score:\n",
      "    Income   z_score     mod_z\n",
      "7     100  1.202258  3.665761\n",
      "8     110  1.466491  4.252283\n",
      "9     115  1.598607  4.545543\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Income': [22, 25, 27, 29, 35, 40, 42, 100, 110, 115]})\n",
    "\n",
    "# IQR \n",
    "\n",
    "# Stpes to calculate Quartiles: \n",
    "# 1. Sort the data: [22, 25, 27, 29, 35, 40, 42, 100, 110, 115]\n",
    "# 2. Find the position in the sorted array for the quantile: (n-1) * quantile, where n is the number of data points and quantile is 0.25 for Q1 and 0.75 for Q3.\n",
    "#   - For Q1: (10-1) * 0.25 = 2.25 → between 2nd and 3rd values (27 and 29)\n",
    "#   - For Q3: (10-1) * 0.75 = 6.75 → between 6th and 7th values (42 and 100)\n",
    "# 3. Interpolate between these values if necessary\n",
    "#   - Q1 = 27 + 0.25 * (29 - 27) = 27.5\n",
    "#   - Q3 = 42 + 0.75 * (100 - 42) = 87.5\n",
    "\n",
    "Q1 = df['Income'].quantile(0.25)\n",
    "Q3 = df['Income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(\"Q1:\", Q1, \"Q3:\", Q3, \"IQR:\", IQR)\n",
    "\n",
    "lower = Q1 - 1.0 * IQR\n",
    "upper = Q3 + 1.0 * IQR\n",
    "print(\"Lower Bound:\", lower, \"Upper Bound:\", upper)\n",
    "outliers_iqr = df[(df['Income'] < lower) | (df['Income'] > upper)]      # Outliers should be outside the lower and upper bounds\n",
    "\n",
    "print(\"Outliers using IQR:\\n\", outliers_iqr)\n",
    "\n",
    "# Z-score \n",
    "mean = df['Income'].mean()\n",
    "std = df['Income'].std()\n",
    "df['z_score'] = (df['Income'] - mean) / std\n",
    "outliers_z = df[np.abs(df['z_score']) > 2.5]        # Common threshold is 2.5 or 3\n",
    "print(\"Outliers using Z-score:\\n\", outliers_z)\n",
    "\n",
    "# Modified Z-score (Robust)\n",
    "median = df['Income'].median()\n",
    "mad = np.median(np.abs(df['Income'] - median))\n",
    "df['mod_z'] = 0.6745 * (df['Income'] - median) / mad\n",
    "outliers_mz = df[np.abs(df['mod_z']) > 3.5]     # Common threshold is 3.5\n",
    "print(\"Outliers using Modified Z-score:\\n\", outliers_mz)\n",
    "\n",
    "# Here ww can see that, the outlier detection methods (IQR, Z-score, Modified Z-score) identify the high income values (100, 110, 115) as outliers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff6fb0",
   "metadata": {},
   "source": [
    "## Distribution Shapes for ML: Symmetric vs Skewed, Long Tails, and IQR Outliers\n",
    "\n",
    "### Symmetric Distributions\n",
    "A symmetric distribution looks the same on both sides of its center. The most common example is the Normal Distribution. Here, the mean, median, and mode all lie at the center not need to be the same.\n",
    "\n",
    "Many ML algorithms, such as Linear Regression, SVMs, and KNN, assume features follow a roughly normal distribution for optimal performance. If data is symmetric, we often don’t need transformations.\n",
    "\n",
    "### Skewed Distributions\n",
    "A skewed distribution is not symmetric. It can be positively skewed (right-skewed) or negatively skewed (left-skewed).\n",
    "\n",
    "In right-skewed data, the frequency of higher values tapers off more slowly than lower values, creating a long tail on the right side, means the bulk of the data is concentrated on the left. In left-skewed data, the opposite occurs, with a long tail on the left side or data concentrated on the right.\n",
    "In right -skewed distributions, the mean is typically greater than the median, which is greater than the mode (Mean > Median > Mode). In left-skewed distributions, the mean is less than the median, which is less than the mode (Mean < Median < Mode). \n",
    "\n",
    "Right-skewed data often appears in income, price, or reaction time datasets. For ML models, skewness can reduce model accuracy and cause bias. To fix this, we often apply log transformation, Box–Cox, or Yeo–Johnson transformations to make data more symmetric.\n",
    "\n",
    "### Long Tails\n",
    "A long tail means extreme values stretch far from the center. Long-tailed distributions are common in real-world datasets - like user engagement or social media followers — where a few points dominate the range.\n",
    "\n",
    "Long-tailed data challenges ML models. Models may overfit to frequent cases and ignore rare but important events — like fraud detection or rare disease prediction. Handling long tails often involves data normalization, resampling, or outlier-aware models.\n",
    "\n",
    "| Concept            | Description                                      | Example Features in ML                     | Handling Techniques                      |\n",
    "|--------------------|--------------------------------------------------|--------------------------------------------|------------------------------------------|\n",
    "| Symmetric Distribution | Data is evenly distributed around the center.    | Height, weight, test scores                | Often no transformation needed.         |\n",
    "| Skewed Distribution   | Data is unevenly distributed, with a long tail. Which side the data lean toward | Income, house prices, reaction times       | Log transformation, Box-Cox, Yeo–Johnson |\n",
    "| Long Tails          | Extreme values stretch far from the center or how extended rare or extreme values are.      | User engagement, social media followers        | Normalization, resampling, outlier-aware models |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebd87d",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Outliers are data points that differ significantly from most observations. They can distort model training and degrade performance. A simple yet effective method to detect outliers is using the Interquartile Range (IQR)\n",
    "#### When to Keep vs. Remove Outliers?\n",
    "Not all outliers are bad. In some ML problems, like fraud detection or medical diagnosis, outliers represent critical cases. Instead of removing them, we may label or model them separately.\n",
    "\n",
    "**Example:**\n",
    "- In a credit card fraud detection dataset, fraudulent transactions are outliers. Removing them would eliminate crucial information needed to train an effective model.\n",
    "- In a medical dataset, patients with rare diseases may appear as outliers. These cases are vital for diagnosis and treatment, so they should be retained and analyzed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8046e4",
   "metadata": {},
   "source": [
    "#### Why Mean and SD Fail Sometimes?\n",
    "Mean and standard deviation work great for symmetric, normally distributed data - like heights or IQ scores in large populations.\n",
    "But if outliers are present for example, one billionaire in an income dataset - the mean gets pulled toward the extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c29b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 650.625, Median: 29.5, Standard Deviation: 1643.9115652537396\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = [25, 27, 28, 29, 30, 31, 35, 5000]  # Example dataset with an outlier 5000\n",
    "mean = np.mean(data)\n",
    "median = np.median(data)\n",
    "std_dev = np.std(data)\n",
    "print(f\"Mean: {mean}, Median: {median}, Standard Deviation: {std_dev}\")\n",
    "\n",
    "# The mean shoots up, but the median stays stable — that’s why we call median a robust measure of central tendency!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebd3b3",
   "metadata": {},
   "source": [
    "#### When to Use Median and IQR?\n",
    "Use Median and IQR when:\n",
    "1. Data are skewed or have long tails\n",
    "    - Example: Income, house prices, YouTube views, medical costs.\n",
    "    - These are right-skewed, and the mean gets dragged upward by a few large values.\n",
    "    - The median gives a better picture of a “typical” case.\n",
    "2. There are outliers or extreme values\n",
    "    - Outliers inflate SD and make the spread look huge even if most data are normal.\n",
    "    - IQR ignores extremes by focusing on the middle 50%.\n",
    "3. Need robust statistics for ML preprocessing\n",
    "    - Algorithms like RobustScaler in sklearn use median and IQR instead of mean and SD to scale data safely.\n",
    "    - It’s perfect for datasets where outliers shouldn’t dominate scaling.\n",
    "\n",
    "#### When Mean and SD Are Still Better?\n",
    "Use Mean and SD when:\n",
    "- Data are roughly normal (bell-shaped)\n",
    "- Plan to apply algorithms assuming normality - for instance, Linear Regression, Naive Bayes, or Z-score scaling\n",
    "\n",
    "**So to summarize**:\n",
    "- Mean & SD → Best for normal, clean data.\n",
    "- Median & IQR → Best for skewed, messy, or outlier-heavy data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
